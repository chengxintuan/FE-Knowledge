# 冯·诺依曼体系结构：计算机组成的金字塔

**计算机的基本硬件组成**

计算机组成的三大件：CPU、内存和主板。

CPU负责计算，内存存储数据，主板提供插槽，CPU和内存要插在主板上。

主板的芯片组和总线解决了CPU和内存之间如何通信的问题。

芯片组控制了数据传输的流转，也就是数据从哪里到哪里的问题。

总线则是实际数据传输的高速公路。因此，总线速度决定了数据能传输得多快。

有了三大件，只要配上电源供电，计算机差不多就可以跑起来了。

当然，我们还需要各自I/O设备，比如鼠标、键盘、显示器。

还有一个很特殊的设备，就是显卡，之所以特殊，是因为显卡里有除了CPU之外的另一个“处理器”，也就是GPU，GPU一样可以做各种“计算”的工作。

主板上的南桥跟北桥，南桥的作用是来连接鼠标、键盘以及硬盘这些外部设备和CPU之间的通信。

北桥的作用，是连接CPU和内存、显卡之间的通信。目前“北桥”芯片的工作，已经被移到了CPU的内部。

**冯·诺依曼体系结构**

任何一台计算机的任何一个部件都可以归到运算器、控制器、存储器、输入设备和输出设备中，而所有的现代计算机也都是基于这个基础架构来设计开发的。

而所有的计算机程序，也都可以抽象为从输入设备读取输入信息，通过运算器和控制器来执行存储在存储器里的程序，最终把结果输出到输出设备中。而我们所有撰写的无论高级还是低级语言的程序，也都是基于这样一个抽象框架来进行运作的。

**总结延伸**

冯·诺依曼体系结构确立了我们现在每天使用的计算机硬件的基础架构。因此，学习计算机组成原理，其实就是学习和拆解冯·诺依曼体系结构。

具体来说，学习组成原理，其实就是学习控制器、运算器的工作原理，也就是CPU是怎么工作的，以及为何这样设计；学习内存的工作原理，从最基本的电路，到上层抽象给到CPU乃至应用程序的接口是怎样的；学习CPU是怎么和输入设备、输出设备打交道的。

学习组成原理，就是在理解从控制器、运算器、存储器、输入设备以及输出设备，从电路这样的硬件，到最终开放给软件的接口，是怎么运作的，为什么要设计成这样，以及在软件开发层面怎么尽可能用好它。

# 给你一张知识地图，计算机组成原理应该这么学

<br/>
<img src='../../images/367.jpg' width='1000'>
<br/>

# 通过你的CPU主频，我们来谈谈“性能”究竟是什么？

计算机的性能，主要有两个指标。

第一个是响应时间 或者叫执行时间。想要提升响应时间这个性能指标，你可以理解为让计算机“跑得更快”。

第二个是吞吐率或者带宽，想要提升这个指标，你可以理解为让计算机“搬得更多”。

提升吞吐率的办法有很多。大部分时候，我们只要多加一些机器，多堆一些硬件就好了。但是响应时间的提升却没有那么容易。

程序的CPU执行时间 = CPU时钟周期数 × 时钟周期时间

时钟周期时间取决于硬件，软件工程师要重点关注CPU时钟周期数。

对于CPU时钟周期数，我们可以再做一个分解，把它变成`指令数 × 每条指令的平均时钟周期数（简称CPI）`

所以，程序的CPU执行时间 = 指令数 × CPI × 时钟周期时间

要解决性能问题，其实就是要优化这三者。

1. 时钟周期时间，就是计算机主频，这个取决于计算机硬件。
2. 每条指令的平均时钟周期数CPI，就是一条指令到底需要多少CPU Cycle。
3. 指令数，代表执行我们的程序到底需要多少条指令、用哪些指令。

# 穿越功耗墙，我们该从哪些方面提升“性能”？

我们知道CPU的性能，取决于：程序的CPU执行时间 = 指令数 × CPI × 时钟周期时间

1978年到2000年里，提升CPU性能的主要方式是不断提升CPU的时钟频率。

主要是两个措施：增加密度、提升主频。

增加密度，就是同样的面积里面，多放一些晶体管。

提升主频，就是让晶体管“打开”和“关闭”得更快一点。

而这两者，都会增加功耗，带来耗电和散热的问题。这也是限制CPU主频提升的主要原因。

一个CPU的功率，可以用这样一个公式来表示：

`功耗 ~= 1/2 ×负载电容×电压的平方×开关频率×晶体管数量`

为了要提升性能，我们需要不断地增加晶体管数量。同样的面积下，我们想要多放一点晶体管，就要把晶体管造得小一点。这个就是平时我们所说的提升“制程”。

从28nm到7nm，相当于晶体管本身变成了原来的1/4大小。这个就相当于我们在工厂里，同样的活儿，我们要找瘦小一点的工人，这样一个工厂里面就可以多一些人。我们还要提升主频，让开关的频率变快，也就是要找手脚更快的工人。

但是，功耗增加太多，就会导致CPU散热跟不上，这时，我们就需要降低电压。

事实上，从5MHz主频的8086到5GHz主频的Intel i9，CPU的电压已经从5V左右下降到了1V左右。这也是为什么我们CPU的主频提升了1000倍，但是功耗只增长了40倍。

制程的优化和电压的下降，是通过提升主频提升性能。到从奔腾4开始，这个提升已经到了极限。

上一讲，我们知道影响性能有两个指标：响应时间跟吞吐量。

既然响应时间改变不了，就提升吞吐量。

我们现在用的2核、4核，乃至8核的CPU，性能也就成倍提升。

提升响应时间，就好比提升你用的交通工具的速度，比如原本你是开汽车，现在变成了火车乃至飞机。本来开车从上海到北京要20个小时，换成飞机就只要2个小时了，但是，在此之上，再想要提升速度就不太容易了。我们的CPU在奔腾4的年代，就好比已经到了飞机这个速度极限。

那你可能要问了，接下来该怎么办呢？相比于给飞机提速，工程师们又想到了新的办法，可以一次同时开2架、4架乃至8架飞机。虽然从上海到北京的时间没有变，但是一次飞8架飞机能够运的东西自然就变多了，也就是所谓的“吞吐率”变大了。

这也是一个最常见的提升性能的方式，通过并行提高性能。

# 计算机指令

CPU就是计算机的大脑，中文是中央处理器。

从硬件的角度来看，CPU就是一个超大规模集成电路，通过电路实现了加法、乘法乃至各种各样的处理逻辑。

从软件的角度来讲，CPU就是一个执行各种计算机指令的逻辑机器。我们把计算机指令，叫作机器语言。

高级语言，需要先翻译成汇编语言，最后翻译成机器码。

之所以需要先编译成汇编代码，是因为汇编代码是给人看的。

我们日常用的Intel CPU，有2000条左右的CPU指令，常见的指令可以分成五大类。

<br/>
<img src='../../images/368.jpeg' width='800'>
<br/>

不同的CPU有不同的指令集，也就对应着不同的汇编语言和不同的机器码。

要理解机器码的计算方式，可以参考MIPS指令集，看看机器码是如何生成的。这里不赘述。

# CPU是如何执行指令的？

逻辑上，CPU其实就是由一堆寄存器组成的。而寄存器就是CPU内部，由多个触发器或者锁存器组成的简单电路。

寄存器功能各异，有指令地址寄存器、指令寄存器、条件码寄存器等。

实际上，一个程序执行的时候，CPU会根据PC寄存器里的地址，从内存里面把需要执行的指令读取到指令寄存器里面执行，然后根据指令长度自增，开始顺序读取下一条指令。可以看到，一个程序的一条条指令，在内存里面是连续保存的，也会一条条顺序加载。

而有些特殊指令，比如跳转指令，会修改PC寄存器里面的地址值。这样，下一条要执行的指令就不是从内存里面顺序加载的了。

事实上，这些跳转指令的存在，也是我们可以在写程序的时候，使用if…else条件语句和while/for循环语句的原因。

我们讲打孔卡的时候说到，读取打孔卡的机器会顺序地一段一段地读取指令，然后执行。执行完一条指令，它会自动地顺序读取下一条指令。如果执行的当前指令带有跳转的地址，比如往后跳10个指令，那么机器会自动将卡片带往后移动10个指令的位置，再来执行指令。同样的，机器也能向前移动，去读取之前已经执行过的指令。这也就是我们的while/for循环实现的原理。

虽然我们可以用高级语言，可以用不同的语法，比如 if…else 这样的条件分支，或者 while/for 这样的循环方式，来实现不用的程序运行流程，但是回归到计算机可以识别的机器指令级别，其实都只是一个简单的地址跳转而已，也就是一个类似于goto的语句。

想要在硬件层面实现这个goto语句，除了本身需要用来保存下一条指令地址，以及当前正要执行指令的PC寄存器、指令寄存器外，我们只需要再增加一个条件码寄存器，来保留条件判断的状态。这样简简单单的三个寄存器，就可以实现条件判断和循环重复执行代码的功能。

# 函数调用

函数调用和上一节我们讲的if…else和for/while循环有点像。它们两个都是在原来顺序执行的指令过程里，执行了一个内存地址的跳转指令，让指令从原来顺序执行的过程里跳开，从新的跳转后的位置开始执行。

但是，这两个跳转有个区别，if…else和for/while的跳转，是跳转走了就不再回来了，就在跳转后的新地址开始顺序地执行指令，而函数调用的跳转，在对应函数的指令执行完了之后，还要再回到函数调用的地方，继续执行call之后的指令。

我们在内存里面开辟一段空间，用栈这个后进先出的数据结构。栈就像一个乒乓球桶，每次程序调用函数之前，我们都把调用返回后的地址写在一个乒乓球上，然后塞进这个球桶。这个操作其实就是我们常说的压栈。如果函数执行完了，我们就从球桶里取出最上面的那个乒乓球，很显然，这就是出栈。

拿到出栈的乒乓球，找到上面的地址，把程序跳转过去，就返回到了函数调用后的下一条指令了。如果函数A在执行完成之前又调用了函数B，那么在取出乒乓球之前，我们需要往球桶里塞一个乒乓球。

在真实的程序里，压栈的不只有函数调用完成后的返回地址。

比如函数A在调用B的时候，需要传输一些参数数据，这些参数数据在寄存器不够用的时候也会被压入栈中。整个函数A所占用的所有内存空间，就是函数A的栈帧，放在了栈里面。

让函数A调用自己，并且不设任何终止条件。这样一个无限递归的程序，在不断地压栈过程中，将整个栈空间填满，就会造成 stack overflow。

# ELF和静态链接：为什么程序无法同时在Linux和Windows下运行？

高级语言可以通过编译器编译成汇编代码，然后汇编代码再通过汇编器变成CPU可以理解的机器码，于是CPU就可以执行这些机器码了。

但是这个描述把过程大大简化了。下面，我们一起具体来看，C语言程序是如何变成一个可执行程序的。

实际上，“C语言代码-汇编代码-机器码” 这个过程，在我们的计算机上进行的时候是由两部分组成的。

第一个部分由编译（Compile）、汇编（Assemble）以及链接（Link）三个阶段组成。在这三个阶段完成之后，我们就生成了一个可执行文件。

第二部分，我们通过装载器（Loader）把可执行文件装载（Load）到内存中。CPU从内存中读取指令和数据，来开始真正执行程序。

<br/>
<img src='../../images/369.jpg' width='800'>
<br/>

在Linux下，可执行文件和目标文件所使用的都是一种叫ELF的文件格式，中文名字叫可执行与可链接文件格式，这里面不仅存放了编译成的汇编指令，还保留了很多别的数据。

链接器会扫描所有输入的目标文件，然后把所有符号表里的信息收集起来，构成一个全局的符号表。

符号表相当于一个地址簿，把函数名称、变量名称和地址关联了起来。

然后再根据重定位表，把所有不确定要跳转地址的代码，根据符号表里面存储的地址，进行一次修正。最后，把所有的目标文件的对应段进行一次合并，变成了最终的可执行代码。

在链接器把程序变成可执行文件之后，要装载器去执行程序就容易多了。装载器不再需要考虑地址跳转的问题，只需要解析 ELF 文件，把对应的指令和数据，加载到内存里面供CPU执行就可以了。

在Linux下可以执行而在Windows下不能执行了。其中一个非常重要的原因就是，两个操作系统下可执行文件的格式不一样。

Linux下的ELF文件格式，而Windows的可执行文件格式是一种叫作PE的文件格式。

Linux下著名的开源项目Wine，就是通过兼容PE格式的装载器，使得我们能直接在Linux下运行Windows程序的。而现在微软的Windows里面也提供了WSL。

我们去写可以用的程序，也不仅仅是把所有代码放在一个文件里来编译执行，而是可以拆分成不同的函数库，最后通过一个静态链接的机制，使得不同的文件之间既有分工，又能通过静态链接来“合作”，变成一个可执行的程序。

对于ELF格式的文件，为了能够实现这样一个静态链接的机制，里面不只是简单罗列了程序所需要执行的指令，还会包括链接所需要的重定位表和符号表。
